{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1YDpSNu69ymYWKSuTsxRngXXV0CnZGLpM","authorship_tag":"ABX9TyPmGGWh1rLv19lpT6lgdipN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install mediapipe==0.10.20\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"aznBoE1eg9Oc","executionInfo":{"status":"ok","timestamp":1763462395172,"user_tz":-330,"elapsed":38571,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}},"outputId":"a329c225-12a5-4ff0-fcaf-10056b64334b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mediapipe==0.10.20\n","  Downloading mediapipe-0.10.20-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (25.4.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (25.9.23)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (0.7.2)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (0.7.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (3.10.0)\n","Collecting numpy<2 (from mediapipe==0.10.20)\n","  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (4.12.0.88)\n","Collecting protobuf<5,>=4.25.3 (from mediapipe==0.10.20)\n","  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting sounddevice>=0.4.4 (from mediapipe==0.10.20)\n","  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (0.2.1)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.20) (2.0.0)\n","Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.20) (0.5.3)\n","INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n","Collecting jax (from mediapipe==0.10.20)\n","  Downloading jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n","Collecting jaxlib (from mediapipe==0.10.20)\n","  Downloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n","Collecting jax (from mediapipe==0.10.20)\n","  Downloading jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n","Collecting jaxlib (from mediapipe==0.10.20)\n","  Downloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.20) (3.4.0)\n","Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.20) (1.16.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (2.9.0.post0)\n","INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n","Collecting opencv-contrib-python (from mediapipe==0.10.20)\n","  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.20) (2.23)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.20) (1.17.0)\n","Downloading mediapipe-0.10.20-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n","Downloading jax-0.7.1-py3-none-any.whl (2.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl (81.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: protobuf, numpy, sounddevice, opencv-contrib-python, jaxlib, jax, mediapipe\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.5\n","    Uninstalling protobuf-5.29.5:\n","      Successfully uninstalled protobuf-5.29.5\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: opencv-contrib-python\n","    Found existing installation: opencv-contrib-python 4.12.0.88\n","    Uninstalling opencv-contrib-python-4.12.0.88:\n","      Successfully uninstalled opencv-contrib-python-4.12.0.88\n","  Attempting uninstall: jaxlib\n","    Found existing installation: jaxlib 0.7.2\n","    Uninstalling jaxlib-0.7.2:\n","      Successfully uninstalled jaxlib-0.7.2\n","  Attempting uninstall: jax\n","    Found existing installation: jax 0.7.2\n","    Uninstalling jax-0.7.2:\n","      Successfully uninstalled jax-0.7.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n","pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed jax-0.7.1 jaxlib-0.7.1 mediapipe-0.10.20 numpy-1.26.4 opencv-contrib-python-4.11.0.86 protobuf-4.25.8 sounddevice-0.5.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["cv2","google","numpy"]},"id":"14a3e2e78e1e4085a3464f38748eb12c"}},"metadata":{}}]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UWfw2t7WIMD5","executionInfo":{"status":"ok","timestamp":1763462547787,"user_tz":-330,"elapsed":15387,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}},"outputId":"fe8bd4d4-365a-430c-9890-c58e7f0fc197"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.2 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.\n","  warnings.warn(\n"]}],"source":["import os, sys, math, random, shutil, zipfile, glob, time\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","import cv2\n","from tqdm import tqdm\n","import mediapipe as mp\n","import tensorflow as tf\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","from collections import Counter, defaultdict"]},{"cell_type":"code","source":[],"metadata":{"id":"yLLlZSXigiLy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ZIP_VIDEOS = \"/content/drive/MyDrive/Colab Notebooks/words_31.zip\"    # path to zip containing all videos (flat)\n","CSV_PATH    = \"/content/adjectives_32.csv\"  # path to your adjectives_32.csv (auto-detected)\n","OUT_ROOT    = \"/content/pipeline_output\"  # results folder\n","FPS         = 10        # frames per second to extract\n","T_SEQ       = 32        # frames per sequence\n","MIN_CLASS_SAMPLES = 1   # keep >=1 samples per class for now (you can bump to 5 if you want)\n","SEED = 42\n","BATCH_SIZE = 32\n","EPOCHS = 30\n","LR = 3e-4"],"metadata":{"id":"UO5dI0ydi5X8","executionInfo":{"status":"ok","timestamp":1763462981738,"user_tz":-330,"elapsed":4,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["random.seed(SEED)\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)\n","os.makedirs(OUT_ROOT, exist_ok=True)\n","\n","print(\"Config:\")\n","print(\" ZIP_VIDEOS:\", ZIP_VIDEOS)\n","print(\" CSV_PATH   :\", CSV_PATH)\n","print(\" OUT_ROOT   :\", OUT_ROOT)\n","print(\" FPS        :\", FPS, \"T_SEQ:\", T_SEQ)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DfFyGsEYi8pu","executionInfo":{"status":"ok","timestamp":1763462984533,"user_tz":-330,"elapsed":60,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}},"outputId":"0c1a8361-90b7-49be-fa7c-a2c94ae1832d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Config:\n"," ZIP_VIDEOS: /content/drive/MyDrive/Colab Notebooks/words_31.zip\n"," CSV_PATH   : /content/adjectives_32.csv\n"," OUT_ROOT   : /content/pipeline_output\n"," FPS        : 10 T_SEQ: 32\n"]}]},{"cell_type":"code","source":["# ------------------- 1) Unzip videos (if zipped) -------------------\n","VIDEOS_DIR = os.path.join(OUT_ROOT, \"videos\")\n","if os.path.exists(VIDEOS_DIR):\n","    print(\"Videos dir exists, using:\", VIDEOS_DIR)\n","else:\n","    os.makedirs(VIDEOS_DIR, exist_ok=True)\n","    if os.path.exists(ZIP_VIDEOS):\n","        print(\"Unzipping videos...\")\n","        with zipfile.ZipFile(ZIP_VIDEOS, 'r') as z:\n","            z.extractall(VIDEOS_DIR)\n","        print(\"Unzipped to\", VIDEOS_DIR)\n","    else:\n","        raise FileNotFoundError(f\"ZIP_VIDEOS not found: {ZIP_VIDEOS}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y18sscTki_BQ","executionInfo":{"status":"ok","timestamp":1763462758410,"user_tz":-330,"elapsed":191606,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}},"outputId":"57792770-b808-48ab-b2a1-cac93eb47fb5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Unzipping videos...\n","Unzipped to /content/pipeline_output/videos\n"]}]},{"cell_type":"code","source":["# ------------------- 2) Load CSV and auto-detect columns -------------------\n","df_csv = pd.read_csv(CSV_PATH)\n","print(\"CSV loaded. Columns:\", df_csv.columns.tolist())\n","\n","# auto-detect likely column names\n","col_word = None\n","col_singer = None\n","col_file = None\n","for c in df_csv.columns:\n","    cl = c.lower()\n","    if \"word\" in cl or \"word_name\" in cl or \"wordname\" in cl:\n","        col_word = c\n","    if \"singer\" in cl or \"signer\" in cl or \"person\" in cl or \"speaker\" in cl:\n","        col_singer = c\n","    if \"file\" in cl or \"filename\" in cl or \"file_name\" in cl or \"path\" in cl:\n","        col_file = c\n","# if not found, try common names\n","if col_word is None:\n","    for c in df_csv.columns:\n","        if any(k in c.lower() for k in [\"name\",\"label\"]):\n","            col_word = c; break\n","if col_singer is None:\n","    for c in df_csv.columns:\n","        if any(k in c.lower() for k in [\"id\",\"singer\",\"signer\",\"person\"]):\n","            col_singer = c; break\n","if col_file is None:\n","    for c in df_csv.columns:\n","        if any(k in c.lower() for k in [\"file\",\"video\",\"path\",\"fname\"]):\n","            col_file = c; break\n","\n","if not (col_word and col_singer and col_file):\n","    raise ValueError(f\"Could not auto-detect columns. Found: word={col_word}, singer={col_singer}, file={col_file}. CSV columns: {list(df_csv.columns)}\")\n","\n","print(\"Using columns -> word:\", col_word, \"singer:\", col_singer, \"file:\", col_file)\n","df_csv = df_csv[[col_word, col_singer, col_file]].rename(columns={col_word:\"word\", col_singer:\"singer_id\", col_file:\"file_name\"})\n","\n","# Trim whitespace & cast to str\n","df_csv['word'] = df_csv['word'].astype(str).str.strip()\n","df_csv['file_name'] = df_csv['file_name'].astype(str).str.strip()\n","df_csv['singer_id'] = df_csv['singer_id'].astype(str).str.strip()\n","\n","# Build mapping from file_name -> absolute path by searching VIDEOS_DIR (flat)\n","print(\"Indexing video files (this may take a moment)...\")\n","video_index = {}\n","for root, dirs, files in os.walk(VIDEOS_DIR):\n","    for f in files:\n","        if f.lower().endswith(('.mp4','.mov','.avi','.mkv','.webm')):\n","            video_index[f] = os.path.join(root, f)\n","\n","missing = []\n","paths = []\n","for i, r in df_csv.iterrows():\n","    fname = os.path.basename(r.file_name)\n","    if fname in video_index:\n","        paths.append(video_index[fname])\n","    else:\n","        missing.append(fname)\n","        paths.append(None)\n","\n","df_csv['video_path'] = paths\n","n_missing = sum(p is None for p in paths)\n","print(\"Video matches:\", len(df_csv)-n_missing, \"missing:\", n_missing)\n","if n_missing>0:\n","    print(\"First 10 missing examples:\", list(dict.fromkeys(missing))[:10])\n","    # don't fail — keep only matched rows\n","    df_csv = df_csv[df_csv['video_path'].notnull()].reset_index(drop=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lfe35ppwj1JG","executionInfo":{"status":"ok","timestamp":1763462988689,"user_tz":-330,"elapsed":333,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}},"outputId":"8ab1c176-432b-4182-e58c-bd480bb3b421"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["CSV loaded. Columns: ['word_name', 'singer_id', 'file_name']\n","Using columns -> word: word_name singer: singer_id file: file_name\n","Indexing video files (this may take a moment)...\n","Video matches: 651 missing: 0\n"]}]},{"cell_type":"code","source":["# ------------------- 3) Extract frames at FPS -------------------\n","FRAMES_DIR = os.path.join(OUT_ROOT, \"frames\")\n","os.makedirs(FRAMES_DIR, exist_ok=True)\n","\n","def extract_frames_from_video(video_path, out_dir, fps=FPS):\n","    Path(out_dir).mkdir(parents=True, exist_ok=True)\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        print(\"Cannot open\", video_path); return 0\n","    vid_fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n","    frame_interval = max(1, int(round(vid_fps / float(fps))))\n","    total = 0\n","    idx = 0\n","    saved = 0\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        if idx % frame_interval == 0:\n","            # save frame as jpg\n","            fn = os.path.join(out_dir, f\"{Path(video_path).stem}_f{idx:05d}.jpg\")\n","            cv2.imwrite(fn, frame)\n","            saved += 1\n","        idx += 1\n","        total += 1\n","    cap.release()\n","    return saved\n","\n","print(\"Extracting frames for each video (skips if already extracted)...\")\n","rows = []\n","for i, r in tqdm(df_csv.iterrows(), total=len(df_csv)):\n","    vpath = r.video_path\n","    out_sub = os.path.join(FRAMES_DIR, Path(vpath).stem)\n","    if len(glob.glob(os.path.join(out_sub, \"*.jpg\"))) == 0:\n","        n = extract_frames_from_video(vpath, out_sub, fps=FPS)\n","    else:\n","        n = len(glob.glob(os.path.join(out_sub, \"*.jpg\")))\n","    rows.append((r.word, r.singer_id, vpath, out_sub, n))\n","frames_df = pd.DataFrame(rows, columns=[\"word\",\"singer_id\",\"video_path\",\"frames_dir\",\"n_frames\"])\n","print(\"Frames extracted summary:\", frames_df.n_frames.describe())\n","frames_df.to_csv(os.path.join(OUT_ROOT, \"frames_summary.csv\"), index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ldNDx7NJlR7i","executionInfo":{"status":"ok","timestamp":1763463850251,"user_tz":-330,"elapsed":839375,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}},"outputId":"508d70be-8d46-4b54-c2bf-35c91a733c3c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting frames for each video (skips if already extracted)...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 651/651 [13:59<00:00,  1.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Frames extracted summary: count    651.000000\n","mean      30.617512\n","std        6.444767\n","min       18.000000\n","25%       26.000000\n","50%       29.000000\n","75%       34.000000\n","max       55.000000\n","Name: n_frames, dtype: float64\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# ------------------- 4) MediaPipe Holistic extraction per frame -------------------\n","npz_out = os.path.join(OUT_ROOT, \"normalized_npz\")\n","os.makedirs(npz_out, exist_ok=True)\n","manifest_rows = []\n","\n","mp_holistic = mp.solutions.holistic\n","hol = mp_holistic.Holistic(static_image_mode=True, model_complexity=1,\n","                           refine_face_landmarks=False, min_detection_confidence=0.5)\n","\n","def process_image_file(img_path):\n","    img = cv2.imread(img_path)\n","    if img is None:\n","        return None\n","    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    res = hol.process(img_rgb)\n","    return res\n","\n","# helper to return (left21, right21, pose33) each as arrays (x,y,z,vis where vis=1 for hands)\n","def extract_landmarks_array(results):\n","    # left & right hands: 21 points each, with x,y,z; set vis=1 if present\n","    left = np.zeros((21,4), dtype=np.float32)\n","    right = np.zeros((21,4), dtype=np.float32)\n","    pose = np.zeros((33,4), dtype=np.float32)\n","    if results.left_hand_landmarks:\n","        for i,lm in enumerate(results.left_hand_landmarks.landmark):\n","            left[i,:3] = (lm.x, lm.y, lm.z)\n","            left[i,3] = 1.0\n","    if results.right_hand_landmarks:\n","        for i,lm in enumerate(results.right_hand_landmarks.landmark):\n","            right[i,:3] = (lm.x, lm.y, lm.z)\n","            right[i,3] = 1.0\n","    if results.pose_landmarks:\n","        for i,lm in enumerate(results.pose_landmarks.landmark):\n","            pose[i,0:3] = (lm.x, lm.y, lm.z)\n","            pose[i,3] = getattr(lm, \"visibility\", 0.0)\n","    return left, right, pose\n","\n","def normalize_and_build_feature(left, right, pose):\n","    # left: (21,4), right: (21,4), pose: (33,4)\n","    # compute shoulder midpoint (pose indices: left=11, right=12 per MP)\n","    lsh = pose[11,:3].copy()\n","    rsh = pose[12,:3].copy()\n","    # if both zeros, fallback to zeros\n","    if np.allclose(lsh, 0) and np.allclose(rsh,0):\n","        mid = np.zeros(3, dtype=np.float32)\n","        dist = 1.0\n","    else:\n","        mid = (lsh + rsh) / 2.0\n","        dist = np.linalg.norm(lsh - rsh)\n","        if dist < 1e-6:\n","            dist = 1.0\n","    # center & scale\n","    left_xyz = (left[:,:3] - mid) / dist\n","    right_xyz = (right[:,:3] - mid) / dist\n","    pose_xyz = (pose[:,:3] - mid) / dist\n","    # vis columns unchanged (left[:,3], right[:,3], pose[:,3])\n","    # flatten into (21+21+33)*4 = 300 dims (if we include vis). That's x,y,z,vis for each.\n","    left_flat = np.concatenate([left_xyz, left[:,3:4]], axis=1).flatten()\n","    right_flat = np.concatenate([right_xyz, right[:,3:4]], axis=1).flatten()\n","    pose_flat = np.concatenate([pose_xyz, pose[:,3:4]], axis=1).flatten()\n","    base = np.concatenate([left_flat, right_flat, pose_flat])  # 300\n","    # Derived features: inter-hand dist, left open (dist between wrist and index MCP), right open\n","    # Use wrist idx 0 and index_mcp idx 5 (hand landmarks standard)\n","    try:\n","        inter_hand = np.linalg.norm(left_xyz[0] - right_xyz[0])\n","        left_open = np.linalg.norm(left_xyz[0] - left_xyz[5])\n","        right_open = np.linalg.norm(right_xyz[0] - right_xyz[5])\n","    except Exception:\n","        inter_hand, left_open, right_open = 0.0, 0.0, 0.0\n","    derived = np.array([inter_hand, left_open, right_open], dtype=np.float32)\n","    feat = np.concatenate([base, derived]).astype(np.float32)  # 303 dims\n","    # clip to reasonable range\n","    feat = np.clip(feat, -5.0, 5.0)\n","    return feat\n","\n","print(\"Running Holistic on frames and saving normalized_npz (this takes time)...\")\n","total_frames_processed = 0\n","for idx, row in tqdm(frames_df.iterrows(), total=len(frames_df)):\n","    frdir = row.frames_dir\n","    imgs = sorted(glob.glob(os.path.join(frdir, \"*.jpg\")))\n","    for img_path in imgs:\n","        res = process_image_file(img_path)\n","        left, right, pose = extract_landmarks_array(res)\n","        feat = normalize_and_build_feature(left, right, pose)\n","        # save npz named by frame file\n","        out_name = os.path.join(npz_out, Path(img_path).stem + \"_norm.npz\")\n","        np.savez_compressed(out_name, features=feat, word=row.word, singer_id=row.singer_id, orig_image=img_path)\n","        manifest_rows.append((img_path, out_name, row.word, row.singer_id, float(np.mean(feat)), \"ok\"))\n","        total_frames_processed += 1\n","\n","hol.close()\n","print(\"Total frames processed:\", total_frames_processed)\n","\n","manifest_df = pd.DataFrame(manifest_rows, columns=[\"FramePath\",\"normalized_npz\",\"Word\",\"singer_id\",\"feat_mean\",\"status\"])\n","manifest_df.to_csv(os.path.join(OUT_ROOT,\"normalized_manifest.csv\"), index=False)\n","print(\"Saved normalized_manifest.csv with\", len(manifest_df), \"rows.\")\n","\n","# ------------------- Diagnostics (quick) -------------------\n","feats = np.stack([np.load(p)[\"features\"] for p in manifest_df[\"normalized_npz\"].values], axis=0)\n","print(\"Diagnostics: NaN\", np.isnan(feats).sum(), \"Inf\", np.isinf(feats).sum(), \"shape\", feats.shape)\n","print(\"Feat dim:\", feats.shape[1], \"expected 303\")\n","if feats.shape[1] != 303:\n","    print(\"WARNING: feature dim mismatch. Found:\", feats.shape[1], \"expected 303. Adjust preprocess or parser.\")\n","\n","# flag duplicates\n","uniq_cnt = len(np.unique(feats.reshape(feats.shape[0], -1).view([('f', feats.dtype, feats.shape[1])])))\n","print(\"Unique feature vectors:\", uniq_cnt, \"/\", feats.shape[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fznvpHmwlhfy","executionInfo":{"status":"ok","timestamp":1763466267721,"user_tz":-330,"elapsed":2385949,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}},"outputId":"a23862eb-ca61-433a-a54f-214a328e9fde"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Running Holistic on frames and saving normalized_npz (this takes time)...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 651/651 [39:37<00:00,  3.65s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Total frames processed: 19932\n","Saved normalized_manifest.csv with 19932 rows.\n","Diagnostics: NaN 0 Inf 0 shape (19932, 303)\n","Feat dim: 303 expected 303\n","Unique feature vectors: 19892 / 19932\n"]}]},{"cell_type":"code","source":["# ------------------- 5) Build sequences per video (T_SEQ frames) -------------------\n","# Strategy: group frames by video stem (we used frame file stems like video_f00001.jpg)\n","# We'll collect frames per video in temporal order (by frame index in filename).\n","grouped = defaultdict(list)\n","for r in manifest_df.itertuples():\n","    stem = Path(r.FramePath).parent.name  # we saved frames under a folder named by video stem\n","    grouped[stem].append((r.FramePath, r.normalized_npz))\n","\n","# build sequences list (one sequence = up to T_SEQ frames from same video, sliding window stride=T_SEQ)\n","seqs = []\n","labels = []\n","signers = []\n","orig_video_ids = []\n","for vid_stem, items in grouped.items():\n","    # sort by filename\n","    items_sorted = sorted(items, key=lambda x: x[0])\n","    feats_list = [np.load(npz)[\"features\"] for _, npz in items_sorted]\n","    n = len(feats_list)\n","    if n == 0:\n","        continue\n","    # if shorter than T_SEQ -> pad by repeating last frame\n","    if n <= T_SEQ:\n","        arr = np.stack(feats_list + [feats_list[-1]]*(T_SEQ-n), axis=0)\n","        seqs.append(arr)\n","        # assign word, signer from first frame\n","        meta = np.load(items_sorted[0][1])\n","        labels.append(meta[\"word\"].tolist() if isinstance(meta[\"word\"], np.bytes_) else meta[\"word\"])\n","        signers.append(str(meta[\"singer_id\"]))\n","        orig_video_ids.append(vid_stem)\n","    else:\n","        # create non-overlapping chunks (could be sliding; use non-overlapping to avoid leakage)\n","        stride = T_SEQ\n","        for start in range(0, n, stride):\n","            chunk = feats_list[start:start+T_SEQ]\n","            if len(chunk) < T_SEQ:\n","                chunk = chunk + [chunk[-1]]*(T_SEQ - len(chunk))\n","            arr = np.stack(chunk, axis=0)\n","            seqs.append(arr)\n","            meta = np.load(items_sorted[start][1])\n","            labels.append(meta[\"word\"].tolist() if isinstance(meta[\"word\"], np.bytes_) else meta[\"word\"])\n","            signers.append(str(meta[\"singer_id\"]))\n","            orig_video_ids.append(vid_stem)\n","\n","X = np.stack(seqs, axis=0).astype(np.float32)   # shape (N_seq, T_SEQ, F)\n","print(\"Built sequences:\", X.shape)\n","y_words = np.array(labels)\n","signer_arr = np.array(signers)\n","# map words to numeric labels\n","unique_words = sorted(list(set(y_words.tolist())))\n","word_to_idx = {w:i for i,w in enumerate(unique_words)}\n","y = np.array([word_to_idx[w] for w in y_words], dtype=np.int32)\n","print(\"Unique words:\", len(unique_words))\n","\n","# save raw sequences\n","np.save(os.path.join(OUT_ROOT,\"sequences.npy\"), X)\n","np.save(os.path.join(OUT_ROOT,\"labels.npy\"), y)\n","np.save(os.path.join(OUT_ROOT,\"signers.npy\"), signer_arr)\n","pd.DataFrame({\"word\":y_words,\"label\":y,\"signer_id\":signer_arr,\"video_id\":orig_video_ids}).to_csv(os.path.join(OUT_ROOT,\"sequence_manifest.csv\"), index=False)\n","print(\"Saved sequences, labels, signers and manifest.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iElJWWYzo2LK","executionInfo":{"status":"ok","timestamp":1763466671250,"user_tz":-330,"elapsed":10722,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}},"outputId":"749990c1-76b5-49c8-d89b-8f8f3e430fa8"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Built sequences: (874, 32, 303)\n","Unique words: 31\n","Saved sequences, labels, signers and manifest.\n"]}]},{"cell_type":"code","source":["# ------------------- 6) Filter classes with few samples (optional) -------------------\n","counts = Counter(y)\n","print(\"Class sample counts (top 20):\", counts.most_common(20))\n","valid_classes = [cls for cls,cnt in counts.items() if cnt >= MIN_CLASS_SAMPLES]\n","mask = np.isin(y, valid_classes)\n","X = X[mask]\n","y = y[mask]\n","signer_arr = signer_arr[mask]\n","print(\"After filtering classes:\", X.shape, \"classes:\", len(set(y)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jTegXrZNzckl","executionInfo":{"status":"ok","timestamp":1763466685595,"user_tz":-330,"elapsed":47,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}},"outputId":"91ef9da5-a791-44c2-dfca-0c657d6b10b4"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Class sample counts (top 20): [(26, 33), (8, 33), (12, 32), (22, 32), (27, 32), (10, 32), (0, 31), (1, 31), (9, 31), (21, 30), (29, 30), (15, 30), (13, 30), (7, 30), (20, 30), (28, 30), (25, 28), (23, 28), (5, 27), (4, 27)]\n","After filtering classes: (874, 32, 303) classes: 31\n"]}]},{"cell_type":"code","source":["# ------------------- 7) Signer-exclusive split (75/15/10 by signers) -------------------\n","unique_signers = sorted(list(set(signer_arr.tolist())))\n","random.shuffle(unique_signers)\n","n_signers = len(unique_signers)\n","n_train = max(1, int(math.floor(0.75 * n_signers)))\n","n_val = max(1, int(math.floor(0.15 * n_signers)))\n","n_test = max(1, n_signers - n_train - n_val)\n","train_signers = unique_signers[:n_train]\n","val_signers   = unique_signers[n_train:n_train+n_val]\n","test_signers  = unique_signers[n_train+n_val:n_train+n_val+n_test]\n","print(\"Signer counts:\", n_signers, \"train_signers:\", len(train_signers), \"val:\", len(val_signers), \"test:\", len(test_signers))\n","\n","def idxs_for(signers_list):\n","    return np.where(np.isin(signer_arr, signers_list))[0]\n","\n","train_idx = idxs_for(train_signers)\n","val_idx = idxs_for(val_signers)\n","test_idx = idxs_for(test_signers)\n","\n","print(\"Initial split sizes (by sequences):\", len(train_idx), len(val_idx), len(test_idx))\n","# safety: ensure no empty splits\n","if len(val_idx)==0 or len(test_idx)==0:\n","    # fallback to random split by sequences\n","    print(\"Signer split produced empty val/test. Falling back to random stratified by class if possible.\")\n","    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.25, random_state=SEED, stratify=y if len(set(y))>1 else None)\n","    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.4, random_state=SEED, stratify=y_temp if len(set(y_temp))>1 else None)\n","else:\n","    X_train, y_train = X[train_idx], y[train_idx]\n","    X_val,   y_val   = X[val_idx], y[val_idx]\n","    X_test,  y_test  = X[test_idx], y[test_idx]\n","\n","print(\"FINAL SPLITS:\")\n","print(\" Train:\", X_train.shape, y_train.shape)\n","print(\" Val  :\", X_val.shape, y_val.shape)\n","print(\" Test :\", X_test.shape, y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQ8FhqTLzisO","executionInfo":{"status":"ok","timestamp":1763466697346,"user_tz":-330,"elapsed":76,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}},"outputId":"5dc6a3a6-a17d-46d3-ba1a-1da8c4b7acdf"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Signer counts: 7 train_signers: 5 val: 1 test: 1\n","Initial split sizes (by sequences): 542 164 168\n","FINAL SPLITS:\n"," Train: (542, 32, 303) (542,)\n"," Val  : (164, 32, 303) (164,)\n"," Test : (168, 32, 303) (168,)\n"]}]},{"cell_type":"code","source":["# ------------------- 8) Data augmentation helpers (sequence-level) -------------------\n","def spatial_jitter(seq, sigma=0.01):\n","    # seq: (T,F) where F is multiple of 4 usually. Apply small translation to x,y pairs.\n","    T, F = seq.shape\n","    out = seq.copy()\n","    # Determine number of keypoints K = F//4 (if divisible), else best-effort treat as K = F//3\n","    if F % 4 == 0:\n","        K = F//4\n","        mat = out.reshape(T, K, 4)\n","        delta = np.random.uniform(-sigma, sigma, size=(T,1,2))\n","        mat[:,:, :2] = mat[:,:, :2] + delta\n","        out = mat.reshape(T, F)\n","    elif F % 3 == 0:\n","        K = F//3\n","        mat = out.reshape(T, K, 3)\n","        delta = np.random.uniform(-sigma, sigma, size=(T,1,2))\n","        mat[:,:, :2] = mat[:,:, :2] + delta\n","        out = mat.reshape(T, F)\n","    return out\n","\n","def time_warp(seq, low=0.85, high=1.15):\n","    T = seq.shape[0]\n","    r = np.random.uniform(low, high)\n","    new_t = np.linspace(0, T-1, int(round(T*r)))\n","    from scipy.interpolate import interp1d\n","    f = interp1d(np.arange(T), seq, axis=0, kind='linear', fill_value=\"extrapolate\")\n","    warped = f(np.linspace(0, T-1, T))\n","    return warped\n","\n","def drop_frames(seq, max_drop=4):\n","    T = seq.shape[0]\n","    drop_n = random.randint(1, max_drop)\n","    keep_idx = sorted(random.sample(range(T), max(1, T-drop_n)))\n","    kept = seq[keep_idx]\n","    # resample to T\n","    from scipy.interpolate import interp1d\n","    f = interp1d(np.linspace(0,1,len(kept)), kept, axis=0, kind='linear', fill_value=\"extrapolate\")\n","    return f(np.linspace(0,1,T))\n","\n","def augment_sequence(seq):\n","    out = seq.copy()\n","    if random.random() < 0.6:\n","        out = spatial_jitter(out, sigma=0.015)\n","    if random.random() < 0.4:\n","        out = drop_frames(out, max_drop=3)\n","    if random.random() < 0.5:\n","        out = time_warp(out, 0.9, 1.1)\n","    # small gaussian noise\n","    if random.random() < 0.8:\n","        out = out + np.random.normal(0, 0.003, size=out.shape)\n","    return out.astype(np.float32)\n","\n","# ------------------- 9) Simple data generator -------------------\n","class SeqGenerator(keras.utils.Sequence):\n","    def __init__(self, X, y, batch_size=BATCH_SIZE, augment=False, shuffle=True):\n","        self.X = X\n","        self.y = y\n","        self.batch_size = batch_size\n","        self.augment = augment\n","        self.shuffle = shuffle\n","        self.indices = np.arange(len(X))\n","        self.on_epoch_end()\n","    def __len__(self):\n","        return max(1, math.ceil(len(self.X)/self.batch_size))\n","    def __getitem__(self, idx):\n","        batch_idx = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n","        Xb = []\n","        yb = []\n","        for i in batch_idx:\n","            seq = self.X[i]\n","            if self.augment:\n","                seq = augment_sequence(seq)\n","            # flatten sequence to (T*F,)\n","            Xb.append(seq.reshape(-1))\n","            yb.append(self.y[i])\n","        Xb = np.stack(Xb).astype(np.float32)\n","        yb = np.array(yb, dtype=np.int32)\n","        return Xb, yb\n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            np.random.shuffle(self.indices)\n","\n","# Build datasets\n","train_gen = SeqGenerator(X_train, y_train, batch_size=BATCH_SIZE, augment=True, shuffle=True)\n","val_gen   = SeqGenerator(X_val,   y_val,   batch_size=BATCH_SIZE, augment=False, shuffle=False)\n","test_gen  = SeqGenerator(X_test,  y_test,  batch_size=BATCH_SIZE, augment=False, shuffle=False)\n"],"metadata":{"id":"pq4ikUN2zljR","executionInfo":{"status":"ok","timestamp":1763466710962,"user_tz":-330,"elapsed":49,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# ------------------- 10) MLP model for landmarks-only (fast) -------------------\n","input_dim = T_SEQ * X_train.shape[2]\n","num_classes = len(unique_words)\n","def build_mlp(input_dim, num_classes):\n","    inp = keras.layers.Input(shape=(input_dim,))\n","    x = keras.layers.LayerNormalization()(inp)\n","    x = keras.layers.Dense(1024, activation=\"relu\")(x)\n","    x = keras.layers.BatchNormalization()(x)\n","    x = keras.layers.Dropout(0.4)(x)\n","    x = keras.layers.Dense(512, activation=\"relu\")(x)\n","    x = keras.layers.BatchNormalization()(x)\n","    x = keras.layers.Dropout(0.4)(x)\n","    x = keras.layers.Dense(256, activation=\"relu\")(x)\n","    x = keras.layers.Dropout(0.4)(x)\n","    out = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n","    model = keras.Model(inputs=inp, outputs=out)\n","    return model\n","\n","model = build_mlp(input_dim, num_classes)\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=LR),\n","              loss=\"sparse_categorical_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()\n","\n","# class weights to counter class imbalance\n","train_counts = Counter(y_train.tolist())\n","class_weights = {i: (len(y_train)/(num_classes * train_counts.get(i,1))) for i in range(num_classes)}\n","print(\"Class weights computed.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":546},"id":"cK17J6BRzo4_","executionInfo":{"status":"ok","timestamp":1763466733081,"user_tz":-330,"elapsed":1369,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}},"outputId":"80ff477d-ab1b-4a8f-8138-b07df1b9998d"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9696\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ layer_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9696\u001b[0m)           │        \u001b[38;5;34m19,392\u001b[0m │\n","│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m9,929,728\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m)             │         \u001b[38;5;34m7,967\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9696</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ layer_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9696</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,392</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,929,728</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,967</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,619,359\u001b[0m (40.51 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,619,359</span> (40.51 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,616,287\u001b[0m (40.50 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,616,287</span> (40.50 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,072\u001b[0m (12.00 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> (12.00 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Class weights computed.\n"]}]},{"cell_type":"code","source":["# ------------------- 11) Callbacks: save best every epoch -------------------\n","ckpt_dir = os.path.join(OUT_ROOT, \"checkpoints\")\n","os.makedirs(ckpt_dir, exist_ok=True)\n","best_path = os.path.join(ckpt_dir, \"best_model_epoch_{epoch:02d}_valacc_{val_accuracy:.4f}.keras\")\n","# ModelCheckpoint cannot save every epoch with dynamic name directly; implement custom callback\n","class SaveEveryEpochCallback(keras.callbacks.Callback):\n","    def __init__(self, out_pattern):\n","        super().__init__()\n","        self.out_pattern = out_pattern\n","    def on_epoch_end(self, epoch, logs=None):\n","        logs = logs or {}\n","        val_acc = logs.get(\"val_accuracy\", 0.0)\n","        path = self.out_pattern.format(epoch=epoch+1, val_accuracy=val_acc)\n","        self.model.save(path)\n","        print(\"Saved model to\", path)\n","\n","save_cb = SaveEveryEpochCallback(best_path)\n","reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n","# no EarlyStopping as requested\n"],"metadata":{"id":"APOMpqAszt95","executionInfo":{"status":"ok","timestamp":1763466744012,"user_tz":-330,"elapsed":3,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# ------------------- 12) Train -------------------\n","print(\"Starting training. This may take a while.\")\n","history = model.fit(\n","    train_gen,\n","    validation_data=val_gen,\n","    epochs=EPOCHS,\n","    callbacks=[save_cb, reduce_lr],\n","    class_weight=class_weights,\n","    verbose=2\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TgrMQkJ1zw-F","executionInfo":{"status":"ok","timestamp":1763466990100,"user_tz":-330,"elapsed":172180,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}},"outputId":"e4084a1f-5cbf-4be9-b4f4-ffd8bacf1d35"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training. This may take a while.\n","Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Saved model to /content/pipeline_output/checkpoints/best_model_epoch_01_valacc_0.0610.keras\n","17/17 - 11s - 647ms/step - accuracy: 0.0406 - loss: 4.4841 - val_accuracy: 0.0610 - val_loss: 7.0149 - learning_rate: 3.0000e-04\n","Epoch 2/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_02_valacc_0.0671.keras\n","17/17 - 9s - 529ms/step - accuracy: 0.1070 - loss: 3.6779 - val_accuracy: 0.0671 - val_loss: 6.6871 - learning_rate: 3.0000e-04\n","Epoch 3/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_03_valacc_0.0610.keras\n","17/17 - 7s - 394ms/step - accuracy: 0.1421 - loss: 3.3371 - val_accuracy: 0.0610 - val_loss: 5.9843 - learning_rate: 3.0000e-04\n","Epoch 4/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_04_valacc_0.0732.keras\n","17/17 - 5s - 282ms/step - accuracy: 0.1697 - loss: 3.1636 - val_accuracy: 0.0732 - val_loss: 5.4533 - learning_rate: 3.0000e-04\n","Epoch 5/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_05_valacc_0.0671.keras\n","17/17 - 5s - 288ms/step - accuracy: 0.1956 - loss: 3.0126 - val_accuracy: 0.0671 - val_loss: 5.1529 - learning_rate: 3.0000e-04\n","Epoch 6/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_06_valacc_0.0732.keras\n","17/17 - 7s - 408ms/step - accuracy: 0.2362 - loss: 2.8712 - val_accuracy: 0.0732 - val_loss: 5.6985 - learning_rate: 3.0000e-04\n","Epoch 7/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_07_valacc_0.0610.keras\n","17/17 - 5s - 277ms/step - accuracy: 0.2565 - loss: 2.6892 - val_accuracy: 0.0610 - val_loss: 5.5309 - learning_rate: 3.0000e-04\n","Epoch 8/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_08_valacc_0.0549.keras\n","17/17 - 5s - 308ms/step - accuracy: 0.2731 - loss: 2.6187 - val_accuracy: 0.0549 - val_loss: 4.8020 - learning_rate: 3.0000e-04\n","Epoch 9/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_09_valacc_0.0427.keras\n","17/17 - 6s - 345ms/step - accuracy: 0.2731 - loss: 2.5717 - val_accuracy: 0.0427 - val_loss: 4.9687 - learning_rate: 3.0000e-04\n","Epoch 10/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_10_valacc_0.0610.keras\n","17/17 - 4s - 264ms/step - accuracy: 0.2970 - loss: 2.3887 - val_accuracy: 0.0610 - val_loss: 4.6252 - learning_rate: 3.0000e-04\n","Epoch 11/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_11_valacc_0.0549.keras\n","17/17 - 5s - 307ms/step - accuracy: 0.3321 - loss: 2.2898 - val_accuracy: 0.0549 - val_loss: 4.4067 - learning_rate: 3.0000e-04\n","Epoch 12/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_12_valacc_0.0671.keras\n","17/17 - 5s - 318ms/step - accuracy: 0.3985 - loss: 2.0898 - val_accuracy: 0.0671 - val_loss: 4.2628 - learning_rate: 3.0000e-04\n","Epoch 13/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_13_valacc_0.0732.keras\n","17/17 - 4s - 257ms/step - accuracy: 0.4041 - loss: 2.0699 - val_accuracy: 0.0732 - val_loss: 4.2396 - learning_rate: 3.0000e-04\n","Epoch 14/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_14_valacc_0.0427.keras\n","17/17 - 6s - 362ms/step - accuracy: 0.4096 - loss: 2.0793 - val_accuracy: 0.0427 - val_loss: 4.2184 - learning_rate: 3.0000e-04\n","Epoch 15/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_15_valacc_0.0427.keras\n","17/17 - 5s - 278ms/step - accuracy: 0.4244 - loss: 1.9838 - val_accuracy: 0.0427 - val_loss: 4.3481 - learning_rate: 3.0000e-04\n","Epoch 16/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_16_valacc_0.0610.keras\n","17/17 - 5s - 292ms/step - accuracy: 0.4170 - loss: 1.9339 - val_accuracy: 0.0610 - val_loss: 4.1168 - learning_rate: 3.0000e-04\n","Epoch 17/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_17_valacc_0.0366.keras\n","17/17 - 6s - 350ms/step - accuracy: 0.4576 - loss: 1.8641 - val_accuracy: 0.0366 - val_loss: 4.1602 - learning_rate: 3.0000e-04\n","Epoch 18/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_18_valacc_0.0366.keras\n","17/17 - 9s - 510ms/step - accuracy: 0.4428 - loss: 1.9100 - val_accuracy: 0.0366 - val_loss: 4.3593 - learning_rate: 3.0000e-04\n","Epoch 19/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_19_valacc_0.0549.keras\n","17/17 - 7s - 392ms/step - accuracy: 0.4760 - loss: 1.7510 - val_accuracy: 0.0549 - val_loss: 4.1145 - learning_rate: 3.0000e-04\n","Epoch 20/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_20_valacc_0.0793.keras\n","17/17 - 5s - 299ms/step - accuracy: 0.5018 - loss: 1.6918 - val_accuracy: 0.0793 - val_loss: 4.0050 - learning_rate: 3.0000e-04\n","Epoch 21/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_21_valacc_0.0610.keras\n","17/17 - 5s - 272ms/step - accuracy: 0.4779 - loss: 1.7480 - val_accuracy: 0.0610 - val_loss: 3.8267 - learning_rate: 3.0000e-04\n","Epoch 22/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_22_valacc_0.0610.keras\n","17/17 - 6s - 351ms/step - accuracy: 0.4852 - loss: 1.6794 - val_accuracy: 0.0610 - val_loss: 3.8127 - learning_rate: 3.0000e-04\n","Epoch 23/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_23_valacc_0.0549.keras\n","17/17 - 4s - 259ms/step - accuracy: 0.5074 - loss: 1.5510 - val_accuracy: 0.0549 - val_loss: 3.8295 - learning_rate: 3.0000e-04\n","Epoch 24/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_24_valacc_0.0915.keras\n","17/17 - 5s - 320ms/step - accuracy: 0.5148 - loss: 1.5841 - val_accuracy: 0.0915 - val_loss: 3.7829 - learning_rate: 3.0000e-04\n","Epoch 25/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_25_valacc_0.0732.keras\n","17/17 - 5s - 308ms/step - accuracy: 0.5572 - loss: 1.5122 - val_accuracy: 0.0732 - val_loss: 3.7632 - learning_rate: 3.0000e-04\n","Epoch 26/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_26_valacc_0.0732.keras\n","17/17 - 4s - 262ms/step - accuracy: 0.5295 - loss: 1.4818 - val_accuracy: 0.0732 - val_loss: 3.8941 - learning_rate: 3.0000e-04\n","Epoch 27/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_27_valacc_0.0915.keras\n","17/17 - 5s - 289ms/step - accuracy: 0.5351 - loss: 1.5019 - val_accuracy: 0.0915 - val_loss: 3.8919 - learning_rate: 3.0000e-04\n","Epoch 28/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_28_valacc_0.0610.keras\n","\n","Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n","17/17 - 5s - 285ms/step - accuracy: 0.5517 - loss: 1.4595 - val_accuracy: 0.0610 - val_loss: 4.0344 - learning_rate: 3.0000e-04\n","Epoch 29/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_29_valacc_0.0732.keras\n","17/17 - 5s - 275ms/step - accuracy: 0.5738 - loss: 1.3248 - val_accuracy: 0.0732 - val_loss: 3.9190 - learning_rate: 1.5000e-04\n","Epoch 30/30\n","Saved model to /content/pipeline_output/checkpoints/best_model_epoch_30_valacc_0.0793.keras\n","17/17 - 6s - 370ms/step - accuracy: 0.5923 - loss: 1.2977 - val_accuracy: 0.0793 - val_loss: 3.9014 - learning_rate: 1.5000e-04\n"]}]},{"cell_type":"code","source":["# ------------------- 13) Evaluate -------------------\n","print(\"Evaluating on test set:\")\n","test_loss, test_acc = model.evaluate(test_gen, verbose=2)\n","print(\"Test loss:\", test_loss, \"Test acc:\", test_acc)\n","\n","# Predictions and classification report\n","# Get true labels from the test set directly\n","y_true = y_test\n","\n","# Get predictions using the model.predict method with the test generator\n","# This ensures consistency with how evaluate worked and avoids potential re-iteration issues\n","all_predictions = model.predict(test_gen)\n","y_pred = np.argmax(all_predictions, axis=1).tolist()\n","\n","print(\"Classification report (test):\")\n","print(classification_report(y_true, y_pred, target_names=[w for w in unique_words], zero_division=0))\n","cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n","np.save(os.path.join(OUT_ROOT,\"confusion_matrix.npy\"), cm)\n","print(\"Saved confusion matrix and artifacts to:\", OUT_ROOT)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YEcheYmr0DA3","executionInfo":{"status":"ok","timestamp":1763467107535,"user_tz":-330,"elapsed":1208,"user":{"displayName":"Leela Sai Ramanolla","userId":"16022170146802032023"}},"outputId":"2f567280-f227-4163-f5c8-1601fb47c832"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating on test set:\n","6/6 - 0s - 50ms/step - accuracy: 0.1845 - loss: 3.1492\n","Test loss: 3.149167060852051 Test acc: 0.184523805975914\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step\n","Classification report (test):\n","                precision    recall  f1-score   support\n","\n","             I       0.00      0.00      0.00         7\n","       alright       0.25      0.12      0.17         8\n","           bad       0.00      0.00      0.00         3\n","          cold       0.00      0.00      0.00         3\n","           dry       0.33      1.00      0.50         3\n","          fast       0.00      0.00      0.00         3\n","          good       1.00      0.50      0.67         4\n","good afternoon       0.00      0.00      0.00         8\n","  good evening       0.00      0.00      0.00         8\n","  good morning       0.09      0.25      0.13         8\n","    good night       0.20      0.12      0.15         8\n","         happy       0.00      0.00      0.00         3\n","            he       0.00      0.00      0.00         8\n","         hello       0.00      0.00      0.00         6\n","           hot       0.31      1.00      0.47         4\n","            it       0.00      0.00      0.00         8\n","         large       0.75      1.00      0.86         3\n","          loud       0.00      0.00      0.00         3\n","           new       0.27      1.00      0.43         3\n","           old       0.60      1.00      0.75         3\n","       pleased       0.00      0.00      0.00         8\n","         quiet       1.00      0.25      0.40         4\n","           she       0.00      0.00      0.00         8\n","          slow       1.00      0.33      0.50         3\n","         small       0.40      0.67      0.50         3\n","     thank you       0.00      0.00      0.00         8\n","          they       0.17      0.50      0.26         8\n","            we       0.00      0.00      0.00         8\n","           wet       0.33      0.25      0.29         4\n","           you       0.00      0.00      0.00         7\n","         young       0.00      0.00      0.00         3\n","\n","      accuracy                           0.18       168\n","     macro avg       0.22      0.26      0.20       168\n","  weighted avg       0.16      0.18      0.14       168\n","\n","Saved confusion matrix and artifacts to: /content/pipeline_output\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"TNN71xLi00uR"},"execution_count":null,"outputs":[]}]}