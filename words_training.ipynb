{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13656345,"sourceType":"datasetVersion","datasetId":8682107},{"sourceId":13664067,"sourceType":"datasetVersion","datasetId":8687524}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 0 — Fix environment (pin compatible versions) and restart the kernel\n# Run this ONCE. It will restart the Python kernel so the pins take effect.\n\n!pip -q install --upgrade --force-reinstall \\\n  \"numpy==1.26.4\" \"scipy==1.10.1\" \\\n  \"protobuf==4.25.3\" \"mediapipe==0.10.14\" \"decord==0.6.0\" \\\n  timm opencv-python-headless scikit-learn pandas joblib\n\nimport IPython\nIPython.Application.instance().kernel.do_shutdown(restart=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:16:53.309986Z","iopub.execute_input":"2025-11-10T09:16:53.310663Z","iopub.status.idle":"2025-11-10T09:19:58.076300Z","shell.execute_reply.started":"2025-11-10T09:16:53.310641Z","shell.execute_reply":"2025-11-10T09:19:58.075621Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.3 which is incompatible.\nmne 1.10.2 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\nydata-profiling 4.17.0 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.7 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2025.10.0 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nkaggle-environments 1.18.0 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.1.0 which is incompatible.\ntokenizers 0.21.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.1.2 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nscikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.9.0 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ncvxpy 1.6.7 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\ngradio 5.38.1 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\nxarray-einstats 0.9.1 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\ntransformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.1.2 which is incompatible.\nflax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.38 which is incompatible.\ntensorflow 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.3 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\norbax-checkpoint 0.11.19 requires jax>=0.5.0, but you have jax 0.4.38 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"{'status': 'ok', 'restart': True}"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# Cell 1 — Imports and quiet setup (run this AFTER the kernel restarts)\n\nimport os, warnings\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom joblib import Parallel, delayed\nfrom decord import VideoReader, cpu\n\n# Silence native logs\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"]  = \"3\"\nos.environ[\"GLOG_minloglevel\"]      = \"3\"\nos.environ[\"GLOG_logtostderr\"]      = \"1\"\nos.environ[\"MEDIAPIPE_DISABLE_GPU\"] = \"1\"\nwarnings.filterwarnings(\"ignore\")\n\n# Use stable MP solutions path (avoids tasks/audio import issues)\nfrom mediapipe.python.solutions import holistic as mp_holistic\nfrom mediapipe.python.solutions.pose import PoseLandmark as PL\n\nfrom absl import logging as absl_logging\nabsl_logging.set_verbosity(absl_logging.FATAL)\n\n# SciPy (now compatible with pinned NumPy)\nfrom scipy.interpolate import interp1d\nfrom scipy.ndimage import uniform_filter1d\n\n# For silencing native library stderr/stdout in MP calls\nfrom contextlib import contextmanager\n@contextmanager\ndef suppress_output_fd():\n    devnull = open(os.devnull, 'w')\n    old_out, old_err = os.dup(1), os.dup(2)\n    os.dup2(devnull.fileno(), 1)\n    os.dup2(devnull.fileno(), 2)\n    try:\n        yield\n    finally:\n        os.dup2(old_out, 1); os.dup2(old_err, 2)\n        os.close(old_out); os.close(old_err); devnull.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:20:42.120044Z","iopub.execute_input":"2025-11-10T09:20:42.120323Z","iopub.status.idle":"2025-11-10T09:20:57.013060Z","shell.execute_reply.started":"2025-11-10T09:20:42.120302Z","shell.execute_reply":"2025-11-10T09:20:57.012450Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762766444.846782     188 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762766444.904698     188 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2 — Config and manifests\nMANIFEST_FULL = \"/kaggle/input/manifest/manifest_isl_adjectives.csv\"        # train from here\nMANIFEST_EVAL = \"/kaggle/input/manifest/manifest_isl_adjectives_eval.csv\"   # val/test from here\n\nOUT_DIR       = Path(\"/kaggle/working/isl_keypoints\")\n\n# Preprocessing (Step 8)\nINCLUDE_FACE = False\nFPS_TARGET   = 25\nT_TARGET     = 60\nSMOOTH_WIN   = 5\nADD_VELOCITY = True\nN_JOBS       = 2\n\n# Rendering/Training\nSIZE         = 224\nPOSE_STRIDE  = 2\nBATCH        = 4\n\nassert Path(MANIFEST_FULL).exists(), \"Missing manifest_isl_adjectives.csv\"\nassert Path(MANIFEST_EVAL).exists(), \"Missing manifest_isl_adjectives_eval.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:22:07.189459Z","iopub.execute_input":"2025-11-10T09:22:07.190672Z","iopub.status.idle":"2025-11-10T09:22:07.210785Z","shell.execute_reply.started":"2025-11-10T09:22:07.190644Z","shell.execute_reply":"2025-11-10T09:22:07.209956Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Cell 3 — Load manifests (train from FULL; val/test from EVAL)\ndf_full = pd.read_csv(MANIFEST_FULL)\ndf_eval = pd.read_csv(MANIFEST_EVAL)\n\ndf_train = df_full[df_full.split == \"train\"][['path','label','split']].reset_index(drop=True)\ndf_val   = df_eval[df_eval.split == \"val\"][['path','label','split']].reset_index(drop=True)\ndf_test  = df_eval[df_eval.split == \"test\"][['path','label','split']].reset_index(drop=True)\n\nassert set(df_train.path).isdisjoint(set(df_val.path))\nassert set(df_train.path).isdisjoint(set(df_test.path))\nassert set(df_val.path).isdisjoint(set(df_test.path))\n\ndf_all = pd.concat([df_train, df_val, df_test], ignore_index=True).drop_duplicates('path')\nprint(\"Counts | train:\", len(df_train), \"val:\", len(df_val), \"test:\", len(df_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:22:15.517420Z","iopub.execute_input":"2025-11-10T09:22:15.518012Z","iopub.status.idle":"2025-11-10T09:22:15.557500Z","shell.execute_reply.started":"2025-11-10T09:22:15.517986Z","shell.execute_reply":"2025-11-10T09:22:15.556749Z"}},"outputs":[{"name":"stdout","text":"Counts | train: 765 val: 318 test: 309\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 4 — Step 8: preprocess videos -> landmarks -> normalized sequences -> NPZ\ndef sample_frames(path, fps_target=25):\n    vr = VideoReader(path, ctx=cpu(0))\n    n = len(vr)\n    fps_src = float(vr.get_avg_fps()) if hasattr(vr, \"get_avg_fps\") else fps_target\n    step = max(1, int(round(fps_src / fps_target)))\n    idxs = np.arange(0, n, step, dtype=int)\n    if len(idxs) == 0:\n        idxs = np.array([0], dtype=int)\n    return vr.get_batch(idxs).asnumpy()  # (T,H,W,C) RGB\n\nL_SH, R_SH = int(PL.LEFT_SHOULDER), int(PL.RIGHT_SHOULDER)\n\ndef lm_to_np(lms, count):\n    if lms is None:\n        return np.full((count, 3), np.nan, dtype=np.float32)\n    return np.array([[lm.x, lm.y, lm.z] for lm in lms.landmark], dtype=np.float32)\n\ndef extract_landmarks(frames, include_face=False):\n    T = frames.shape[0]\n    pose_list, lh_list, rh_list, face_list = [], [], [], []\n    with suppress_output_fd():\n        with mp_holistic.Holistic(static_image_mode=False, model_complexity=1,\n                                  smooth_landmarks=True, enable_segmentation=False) as holo:\n            for t in range(T):\n                res   = holo.process(frames[t])\n                pose  = lm_to_np(res.pose_landmarks, 33)\n                lhand = lm_to_np(res.left_hand_landmarks, 21)\n                rhand = lm_to_np(res.right_hand_landmarks, 21)\n                face  = lm_to_np(res.face_landmarks, 468) if include_face else np.zeros((0,3), np.float32)\n                pose_list.append(pose); lh_list.append(lhand); rh_list.append(rhand); face_list.append(face)\n    pose = np.stack(pose_list, axis=0); lh = np.stack(lh_list, axis=0); rh = np.stack(rh_list, axis=0); face = np.stack(face_list, axis=0)\n    pose[..., :2] = np.clip(pose[..., :2], 0.0, 1.0)\n    lh[...,   :2] = np.clip(lh[...,   :2], 0.0, 1.0)\n    rh[...,   :2] = np.clip(rh[...,   :2], 0.0, 1.0)\n    if include_face:\n        face[..., :2] = np.clip(face[..., :2], 0.0, 1.0)\n    return pose, lh, rh, face\n\ndef normalize_geometry(pose, lh, rh, face):\n    lsh = pose[:, L_SH, :2]; rsh = pose[:, R_SH, :2]\n    center = (lsh + rsh) / 2.0\n    scale  = np.linalg.norm(lsh - rsh, axis=1, keepdims=True)\n    scale[scale < 1e-6] = 1.0\n    def center_scale(arr):\n        if arr.shape[1] == 0: return arr\n        return np.concatenate([(arr[..., :2] - center[:,None,:]) / scale[:,None,:], arr[..., 2:3]], axis=-1)\n    pose_g = center_scale(pose)\n    lwrist = lh[:, 0:1, :2]; rwrist = rh[:, 0:1, :2]\n    lh_rel = np.concatenate([(lh[..., :2] - lwrist) / scale[:,None,:], lh[..., 2:3]], axis=-1)\n    rh_rel = np.concatenate([(rh[..., :2] - rwrist) / scale[:,None,:], rh[..., 2:3]], axis=-1)\n    face_g = center_scale(face) if face.shape[1] > 0 else face\n    return pose_g, lh_rel, rh_rel, face_g\n\ndef interpolate_nans(seq):\n    T,J,C = seq.shape\n    flat = seq.reshape(T, -1)\n    flat_i = pd.DataFrame(flat).interpolate(method=\"linear\", limit_direction=\"both\", axis=0).values\n    return flat_i.reshape(T, J, C)\n\ndef smooth(seq, win=5):\n    if win <= 1: return seq\n    return uniform_filter1d(seq, size=win, axis=0, mode=\"nearest\")\n\ndef resample_to_T(seq, T_out):\n    T_in = seq.shape[0]\n    if T_in == T_out: return seq\n    x_in  = np.arange(T_in)\n    x_out = np.linspace(0, T_in - 1, T_out)\n    f = interp1d(x_in, seq, kind=\"linear\", axis=0, fill_value=\"extrapolate\", assume_sorted=True)\n    return f(x_out)\n\ndef process_video(path):\n    frames = sample_frames(path, fps_target=FPS_TARGET)\n    pose, lh, rh, face = extract_landmarks(frames, INCLUDE_FACE)\n    pose, lh, rh, face = normalize_geometry(pose, lh, rh, face)\n    seq = np.concatenate([pose, lh, rh, face], axis=1)\n    seq = interpolate_nans(seq)\n    seq = smooth(seq, win=SMOOTH_WIN)\n    seq = resample_to_T(seq, T_out=T_TARGET)\n    X = seq.reshape(T_TARGET, -1)\n    if ADD_VELOCITY:\n        V = np.diff(seq, axis=0, prepend=seq[0:1]).reshape(T_TARGET, -1)\n        X = np.concatenate([X, V], axis=1)\n    return X.astype(np.float32)\n\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\ndef save_npz(row):\n    X = process_video(row['path'])\n    out_path = OUT_DIR / row['split'] / row['label'] / (Path(row['path']).stem + \".npz\")\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    np.savez_compressed(out_path, X=X, label=row['label'], path=row['path'])\n    return str(out_path)\n\nrecords = df_all[['path','label','split']].to_dict('records')\nout_paths = Parallel(n_jobs=N_JOBS, prefer=\"threads\")(delayed(save_npz)(r) for r in records)\n\npd.DataFrame({\"npz_path\": out_paths, \"label\": df_all['label'], \"split\": df_all['split']}).to_csv(OUT_DIR / \"index_all.csv\", index=False)\ndf_train.assign(npz_path=[OUT_DIR / \"train\" / l / (Path(p).stem + \".npz\") for p,l in zip(df_train['path'], df_train['label'])]).to_csv(OUT_DIR / \"index_train.csv\", index=False)\ndf_val.assign(  npz_path=[OUT_DIR / \"val\"   / l / (Path(p).stem + \".npz\") for p,l in zip(df_val['path'],   df_val['label'])]).to_csv(OUT_DIR / \"index_val.csv\",   index=False)\ndf_test.assign( npz_path=[OUT_DIR / \"test\"  / l / (Path(p).stem + \".npz\") for p,l in zip(df_test['path'],  df_test['label'])]).to_csv(OUT_DIR / \"index_test.csv\",  index=False)\n\nprint(\"Step 8 done. NPZ written to:\", OUT_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:22:23.386512Z","iopub.execute_input":"2025-11-10T09:22:23.387084Z","iopub.status.idle":"2025-11-10T10:25:48.337449Z","shell.execute_reply.started":"2025-11-10T09:22:23.387059Z","shell.execute_reply":"2025-11-10T10:25:48.336790Z"}},"outputs":[{"name":"stdout","text":"Step 8 done. NPZ written to: /kaggle/working/isl_keypoints\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 5 — Offline augmentation (TRAIN only) -> index_train_plus_aug.csv\nROOT = OUT_DIR\nTRAIN_INDEX = ROOT / \"index_train.csv\"\nAUG_COPIES  = 1\n\ndef resample_to_T_local(seq, T_out):\n    T_in = seq.shape[0]\n    if T_in == T_out: return seq\n    x_in  = np.arange(T_in)\n    x_out = np.linspace(0, T_in - 1, T_out)\n    f = interp1d(x_in, seq, kind=\"linear\", axis=0, fill_value=\"extrapolate\", assume_sorted=True)\n    return f(x_out)\n\ndef split_coords_vel(X):\n    T, F = X.shape\n    if F % 6 == 0:\n        had_vel = True; Fc = F // 2\n    else:\n        had_vel = False; Fc = F\n    J = Fc // 3\n    coords = X[:, :Fc].reshape(T, J, 3)\n    return coords, had_vel, J\n\ndef reassemble(coords, had_vel):\n    T, J, _ = coords.shape\n    Xc = coords.reshape(T, -1)\n    if had_vel:\n        V = np.diff(coords, axis=0, prepend=coords[0:1]).reshape(T, -1)\n        return np.concatenate([Xc, V], axis=1).astype(np.float32)\n    return Xc.astype(np.float32)\n\ndef augment_once(X):\n    coords, had_vel, J = split_coords_vel(X)\n    if np.random.rand() < 0.5:\n        deg = np.random.uniform(-8, 8); s = np.random.uniform(0.95, 1.05)\n        th = np.deg2rad(deg)\n        R = np.array([[np.cos(th), -np.sin(th)], [np.sin(th), np.cos(th)]], dtype=np.float32)\n        T = coords.shape[0]\n        xy = coords[...,:2].reshape(T*J, 2) @ R.T\n        coords[...,:2] = (xy.reshape(T, J, 2) * s)\n    if np.random.rand() < 0.2:\n        k = max(1, int(round(J * 0.05)))\n        idx = np.random.choice(J, size=k, replace=False)\n        coords[:, idx, :] = 0.0\n    if np.random.rand() < 0.3:\n        T = coords.shape[0]\n        w = max(1, int(round(T * np.random.uniform(0.05, 0.10))))\n        st = np.random.randint(0, max(1, T - w + 1))\n        coords[st:st+w, :, :] = 0.0\n    if np.random.rand() < 0.7:\n        coords += np.random.normal(0.0, 0.01, size=coords.shape).astype(np.float32)\n    coords = resample_to_T_local(coords, T_TARGET)\n    return reassemble(coords, had_vel)\n\ntrain_df = pd.read_csv(TRAIN_INDEX)\nbase = train_df[['npz_path','label']].reset_index(drop=True)\naug_df = pd.DataFrame(np.repeat(base.values, AUG_COPIES, axis=0), columns=base.columns)\n\ndef aug_and_save(npz_path, label):\n    data = np.load(npz_path)\n    X = data['X'].astype(np.float32)\n    X_aug = augment_once(X)\n    src = Path(npz_path)\n    out = src.with_name(src.stem + f\"_aug{np.random.randint(1_000_000):06d}.npz\")\n    out.parent.mkdir(parents=True, exist_ok=True)\n    np.savez_compressed(out, X=X_aug, label=label, path=str(src))\n    return str(out)\n\naug_paths = Parallel(n_jobs=2, prefer=\"threads\")(delayed(aug_and_save)(row['npz_path'], row['label']) for _, row in aug_df.iterrows())\n\ntrain_plus = train_df.copy()\naug_index = pd.DataFrame({\"path\": train_df['path'].iloc[0:len(aug_paths)].values,\n                          \"label\": aug_df['label'].values,\n                          \"split\": \"train\",\n                          \"npz_path\": aug_paths})\ntrain_plus_aug = pd.concat([train_plus, aug_index], ignore_index=True)\ntrain_plus_aug.to_csv(ROOT / \"index_train_plus_aug.csv\", index=False)\n\nprint(\"Augmentation done. Train index:\", ROOT / \"index_train_plus_aug.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:30:04.581752Z","iopub.execute_input":"2025-11-10T10:30:04.582064Z","iopub.status.idle":"2025-11-10T10:30:07.847037Z","shell.execute_reply.started":"2025-11-10T10:30:04.582041Z","shell.execute_reply":"2025-11-10T10:30:07.846423Z"}},"outputs":[{"name":"stdout","text":"Augmentation done. Train index: /kaggle/working/isl_keypoints/index_train_plus_aug.csv\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 6 — Two-stream dataset (landmarks + pose-video rendering)\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nIM_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(1,1,3)\nIM_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(1,1,3)\n\nPOSE_J, LH_J, RH_J = 33, 21, 21\nLH_START, RH_START = POSE_J, POSE_J + LH_J\nPOSE_EDGES = [(11,13),(13,15),(12,14),(14,16),(11,12),(11,23),(12,24),(23,24)]\nHAND_EDGES = [(0,1),(1,2),(2,3),(3,4),\n              (0,5),(5,6),(6,7),(7,8),\n              (0,9),(9,10),(10,11),(11,12),\n              (0,13),(13,14),(14,15),(15,16),\n              (0,17),(17,18),(18,19),(19,20)]\n\ndef split_coords_from_X(X):\n    T, F = X.shape\n    Fc = F // 2 if F % 6 == 0 else F\n    J = Fc // 3\n    coords = X[:, :Fc].reshape(T, J, 3)\n    return coords, J\n\ndef normalize_image(img):\n    arr = img.astype(np.float32) / 255.0\n    return (arr - IM_MEAN) / IM_STD\n\ndef render_skeleton_seq(coords, size=224, stride=2, thickness=2):\n    T, J, _ = coords.shape\n    xy = coords[..., :2]\n    mask = (np.abs(xy).sum(axis=-1) > 0)\n    valid_xy = xy[mask]\n    if valid_xy.size == 0:\n        valid_xy = np.array([[0.0, 0.0]], dtype=np.float32)\n    min_xy = valid_xy.min(axis=0); max_xy = valid_xy.max(axis=0)\n    span = np.maximum(max_xy - min_xy, 1e-3); pad = 0.1 * span\n    min_xy = min_xy - pad; span = (max_xy + pad) - min_xy\n    scale = (size - 1) / np.max(span); offset = -min_xy\n\n    idx = np.arange(0, T, stride, dtype=int)\n    frames = np.zeros((len(idx), size, size, 3), dtype=np.uint8)\n    xy_pix = ((xy + offset) * scale).astype(np.int32)\n\n    t2 = 0\n    for t in idx:\n        canvas = frames[t2]\n        for a, b in POSE_EDGES:\n            if a < J and b < J and mask[t, a] and mask[t, b]:\n                pa = tuple(xy_pix[t, a]); pb = tuple(xy_pix[t, b])\n                cv2.line(canvas, pa, pb, (255,255,255), thickness, cv2.LINE_AA)\n        for a, b in HAND_EDGES:\n            ai, bi = LH_START + a, LH_START + b\n            if ai < J and bi < J and mask[t, ai] and mask[t, bi]:\n                pa = tuple(xy_pix[t, ai]); pb = tuple(xy_pix[t, bi])\n                cv2.line(canvas, pa, pb, (255,255,255), thickness, cv2.LINE_AA)\n        for a, b in HAND_EDGES:\n            ai, bi = RH_START + a, RH_START + b\n            if ai < J and bi < J and mask[t, ai] and mask[t, bi]:\n                pa = tuple(xy_pix[t, ai]); pb = tuple(xy_pix[t, bi])\n                cv2.line(canvas, pa, pb, (255,255,255), thickness, cv2.LINE_AA)\n        frames[t2] = canvas; t2 += 1\n\n    frames = np.stack([normalize_image(frames[k]) for k in range(frames.shape[0])], axis=0)\n    frames = np.transpose(frames, (0,3,1,2))  # (T',3,H,W)\n    return frames.astype(np.float32)\n\nclass TwoStreamDataset(Dataset):\n    def __init__(self, df, label2id, size=224, pose_stride=2):\n        self.df = df.reset_index(drop=True)\n        self.label2id = label2id\n        self.size = size\n        self.pose_stride = pose_stride\n    def __len__(self): return len(self.df)\n    def __getitem__(self, i):\n        row = self.df.iloc[i]\n        data = np.load(row['npz_path'])\n        X = data['X'].astype(np.float32)\n        Xn = (X - X.mean(axis=0, keepdims=True)) / (X.std(axis=0, keepdims=True) + 1e-6)\n        x_land = torch.from_numpy(Xn).transpose(0,1)   # (F,T)\n        coords,_ = split_coords_from_X(X)\n        frames = render_skeleton_seq(coords, size=self.size, stride=self.pose_stride)\n        x_pose = torch.from_numpy(frames)              # (T',3,H,W)\n        y = self.label2id[row['label']]\n        return x_land, x_pose, y\n\ndf_train = pd.read_csv(OUT_DIR / \"index_train_plus_aug.csv\")\ndf_val   = pd.read_csv(OUT_DIR / \"index_val.csv\")\ndf_test  = pd.read_csv(OUT_DIR / \"index_test.csv\")\nlabels = sorted(df_train['label'].unique())\nlabel2id = {l:i for i,l in enumerate(labels)}\ndf_val  = df_val[df_val['label'].isin(labels)].reset_index(drop=True)\ndf_test = df_test[df_test['label'].isin(labels)].reset_index(drop=True)\n\ntrain_ds = TwoStreamDataset(df_train, label2id, size=SIZE, pose_stride=POSE_STRIDE)\nval_ds   = TwoStreamDataset(df_val,   label2id, size=SIZE, pose_stride=POSE_STRIDE)\ntest_ds  = TwoStreamDataset(df_test,  label2id, size=SIZE, pose_stride=POSE_STRIDE)\n\ntrain_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=2, pin_memory=True)\nval_dl   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\ntest_dl  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n\nxb_land, xb_pose, yb = next(iter(train_dl))\nF_in, T_in = xb_land.shape[1], xb_land.shape[2]\nprint(\"Dims | landmarks:\", (F_in, T_in), \"pose batch:\", tuple(xb_pose.shape))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:30:18.445981Z","iopub.execute_input":"2025-11-10T10:30:18.446573Z","iopub.status.idle":"2025-11-10T10:30:24.134037Z","shell.execute_reply.started":"2025-11-10T10:30:18.446539Z","shell.execute_reply":"2025-11-10T10:30:24.133209Z"}},"outputs":[{"name":"stdout","text":"Dims | landmarks: (450, 60) pose batch: (4, 30, 3, 224, 224)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 7 — Two-stream model + fusion\nimport torch\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TemporalAttention(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.W = nn.Linear(d_model, d_model)\n        self.v = nn.Linear(d_model, 1, bias=False)\n    def forward(self, H):  # (T,B,D)\n        M = torch.tanh(self.W(H))\n        e = self.v(M).squeeze(-1)       # (T,B)\n        a = torch.softmax(e, dim=0)     # (T,B)\n        return (a.unsqueeze(-1) * H).sum(dim=0)  # (B,D)\n\nclass LandmarkStream(nn.Module):\n    def __init__(self, f_in, n_classes, k=5, c1=256, c2=256, lstm_hidden=256, dropout=0.4):\n        super().__init__()\n        self.input_bn = nn.BatchNorm1d(f_in)\n        self.conv1 = nn.Conv1d(f_in, c1, kernel_size=k, padding=k//2)\n        self.bn1   = nn.BatchNorm1d(c1)\n        self.drop1 = nn.Dropout(dropout)\n        self.conv2 = nn.Conv1d(c1, c2, kernel_size=k, padding=k//2)\n        self.bn2   = nn.BatchNorm1d(c2)\n        self.drop2 = nn.Dropout(dropout)\n        self.lstm  = nn.LSTM(input_size=c2, hidden_size=lstm_hidden, bidirectional=True, batch_first=False)\n        self.attn  = TemporalAttention(2*lstm_hidden)\n        self.fc1   = nn.Linear(2*lstm_hidden, 512)\n        self.bn3   = nn.BatchNorm1d(512)\n        self.drop3 = nn.Dropout(dropout)\n        self.fc_out= nn.Linear(512, n_classes)\n    def forward(self, x):    # (B,F,T)\n        x = self.input_bn(x)\n        x = self.drop1(F.relu(self.bn1(self.conv1(x))))\n        x = self.drop2(F.relu(self.bn2(self.conv2(x))))\n        H,_ = self.lstm(x.transpose(1,2).transpose(0,1))  # (T,B,2H)\n        h = self.attn(H)                                  # (B,2H)\n        h = self.bn3(self.fc1(h)); h = F.relu(h); h = self.drop3(h)\n        return self.fc_out(h)\n\nclass PoseVideoStream(nn.Module):\n    def __init__(self, n_classes, lstm_hidden=256, dropout=0.4):\n        super().__init__()\n        # MobileNetV2 backbone (ImageNet-pretrained)\n        self.enc  = timm.create_model('mobilenetv2_100', pretrained=True, num_classes=0, global_pool='avg')\n        feat_dim  = self.enc.num_features  # 1280\n        self.lstm = nn.LSTM(input_size=feat_dim, hidden_size=lstm_hidden, bidirectional=True, batch_first=True)\n        self.attn = TemporalAttention(2*lstm_hidden)\n        self.fc1  = nn.Linear(2*lstm_hidden, 512)\n        self.bn   = nn.BatchNorm1d(512)\n        self.drop = nn.Dropout(dropout)\n        self.fc_out = nn.Linear(512, n_classes)\n    def forward(self, x):  # (B,T,3,H,W)\n        B,T,C,H,W = x.shape\n        feats = self.enc(x.reshape(B*T, C, H, W)).view(B, T, -1)  # (B,T,F)\n        Hseq,_ = self.lstm(feats)                                 # (B,T,2H)\n        Hseq = Hseq.transpose(0,1)                                # (T,B,2H)\n        h = self.attn(Hseq)                                       # (B,2H)\n        h = self.bn(self.fc1(h)); h = F.relu(h); h = self.drop(h)\n        return self.fc_out(h)\n\nclass TwoStreamModel(nn.Module):\n    def __init__(self, f_in, n_classes, dropout=0.4):\n        super().__init__()\n        self.land = LandmarkStream(f_in=f_in, n_classes=n_classes, dropout=dropout)\n        self.pose = PoseVideoStream(n_classes=n_classes, dropout=dropout)\n        self.fuse = nn.Linear(2*n_classes, n_classes)  # fuse on concatenated stream logits\n    def forward(self, x_land, x_pose):\n        lg1 = self.land(x_land)     # (B,nc)\n        lg2 = self.pose(x_pose)     # (B,nc)\n        logits = self.fuse(torch.cat([lg1, lg2], dim=-1))\n        return logits, lg1, lg2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:58:55.435304Z","iopub.execute_input":"2025-11-10T10:58:55.435673Z","iopub.status.idle":"2025-11-10T10:58:55.450920Z","shell.execute_reply.started":"2025-11-10T10:58:55.435644Z","shell.execute_reply":"2025-11-10T10:58:55.450151Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Cell 8 — Training utilities (updated: train_epochs now accepts save_path)\nimport torch\nimport numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel  = TwoStreamModel(f_in=F_in, n_classes=len(labels), dropout=0.4).to(device)\n\ndef freeze_bn(m):\n    for mod in m.modules():\n        if isinstance(mod, (nn.BatchNorm1d, nn.BatchNorm2d)):\n            mod.eval()\n            for p in mod.parameters():\n                p.requires_grad = False\n\ndef set_requires_grad(module, flag=True):\n    for p in module.parameters():\n        p.requires_grad = flag\n\ndef params_for_optim(m):\n    return [p for p in m.parameters() if p.requires_grad]\n\ny_train_ids = df_train['label'].map(label2id).values\nclass_w = compute_class_weight(class_weight='balanced', classes=np.arange(len(labels)), y=y_train_ids)\nweights = torch.tensor(class_w, dtype=torch.float32, device=device)\ncriterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.1)\n\ndef evaluate(dl):\n    model.eval(); preds,targs,loss_sum,n = [],[],0.0,0\n    for xl,xp,y in dl:\n        xl=xl.to(device,non_blocking=True); xp=xp.to(device,non_blocking=True); y=torch.as_tensor(y,device=device)\n        with torch.no_grad():\n            logits,_,_ = model(xl,xp); loss = criterion(logits,y)\n        loss_sum += loss.item()*xl.size(0); n += xl.size(0)\n        preds.append(logits.argmax(1).cpu().numpy()); targs.append(y.cpu().numpy())\n    preds=np.concatenate(preds) if preds else np.array([]); targs=np.concatenate(targs) if targs else np.array([])\n    acc=accuracy_score(targs,preds) if len(targs) else 0.0\n    f1 =f1_score(targs,preds,average='macro',zero_division=0) if len(targs) else 0.0\n    return loss_sum/max(1,n), acc, f1\n\ndef train_epochs(dl, val_dl, epochs, lr, wd=5e-4, clip=1.0, cosine=True, desc=\"\", save_path=\"/kaggle/working/best_two_stream.pth\"):\n    optimizer = torch.optim.AdamW(params_for_optim(model), lr=lr, weight_decay=wd)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs) if cosine else None\n    scaler = torch.amp.GradScaler('cuda', enabled=(device.type=='cuda'))\n    best_acc = -1.0\n    for ep in range(1, epochs+1):\n        model.train(); total=0; loss_sum=0.0; preds=[]; targs=[]\n        for xl,xp,y in dl:\n            xl=xl.to(device,non_blocking=True); xp=xp.to(device,non_blocking=True); y=torch.as_tensor(y,device=device)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):\n                logits,_,_ = model(xl,xp); loss = criterion(logits,y)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n            loss_sum += loss.item()*xl.size(0); total += xl.size(0)\n            preds.append(logits.detach().argmax(1).cpu().numpy()); targs.append(y.cpu().numpy())\n        if scheduler is not None: scheduler.step()\n        va_loss, va_acc, va_f1 = evaluate(val_dl)\n        if va_acc > best_acc:\n            best_acc = va_acc\n            torch.save({'model': model.state_dict(), 'labels': labels}, save_path)\n        tr_preds=np.concatenate(preds); tr_targs=np.concatenate(targs)\n        tr_acc=accuracy_score(tr_targs,tr_preds); tr_f1=f1_score(tr_targs,tr_preds,average='macro',zero_division=0)\n        tr_loss=loss_sum/max(1,total)\n        print(f\"{desc} Epoch {ep:02d} | train loss {tr_loss:.4f} acc {tr_acc:.3f} f1 {tr_f1:.3f} | val acc {va_acc:.3f} f1 {va_f1:.3f}\")\n    return best_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:09:34.257699Z","iopub.execute_input":"2025-11-10T11:09:34.257983Z","iopub.status.idle":"2025-11-10T11:09:34.539303Z","shell.execute_reply.started":"2025-11-10T11:09:34.257960Z","shell.execute_reply":"2025-11-10T11:09:34.538702Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Cell 9 — Three training phases (BN frozen by default). Save best per phase and keep overall best.\nimport torch.nn as nn\nimport shutil\n\ntorch.backends.cudnn.benchmark = True\nfreeze_bn(model)\n\nPH1_PATH = \"/kaggle/working/best_two_stream_ph1.pth\"\nPH2_PATH = \"/kaggle/working/best_two_stream_ph2.pth\"\nPH3_PATH = \"/kaggle/working/best_two_stream_ph3.pth\"\nBEST_OVERALL_PATH = \"/kaggle/working/best_two_stream_overall.pth\"\n\n# Phase 1 — warm-up (land: conv2+head; pose: head; fusion)\nset_requires_grad(model, False)\nset_requires_grad(model.land.conv2,  True)\nset_requires_grad(model.land.fc1,    True)\nset_requires_grad(model.land.fc_out, True)\nset_requires_grad(model.pose.fc1,    True)\nset_requires_grad(model.pose.fc_out, True)\nfor p in model.fuse.parameters(): p.requires_grad = True\n\nprint(\"Phase 1: warm-up (land: conv2+head; pose: head), LR=1e-3\")\nbest1 = train_epochs(train_dl, val_dl, epochs=12, lr=1e-3, wd=5e-4, cosine=False, desc=\"[PH1]\", save_path=PH1_PATH)\n\n# Phase 2 — joint (unfreeze selected parts; allow BN updates ONLY in pose encoder)\nfreeze_bn(model)            # keep all BN frozen first\nset_requires_grad(model, False)\n\n# Let BN in the pose encoder update running stats (weights of early blocks still frozen)\nfor m in model.pose.enc.modules():\n    if isinstance(m, nn.BatchNorm2d):\n        m.train()\n        for p in m.parameters():\n            p.requires_grad = True  # will be overridden by next mask for non-final blocks\n\n# Landmark: unfreeze full CNN + LSTM + head\nset_requires_grad(model.land.conv1,  True)\nset_requires_grad(model.land.conv2,  True)\nset_requires_grad(model.land.lstm,   True)\nset_requires_grad(model.land.fc1,    True)\nset_requires_grad(model.land.fc_out, True)\n\n# Pose encoder: train only final blocks (features.14+), plus its LSTM + head\nfor name, p in model.pose.enc.named_parameters():\n    p.requires_grad = any(name.startswith(f\"features.{k}\") for k in [14, 15, 16, 17])\nset_requires_grad(model.pose.lstm,   True)\nset_requires_grad(model.pose.fc1,    True)\nset_requires_grad(model.pose.fc_out, True)\n\nfor p in model.fuse.parameters(): p.requires_grad = True\n\nprint(\"Phase 2: joint (land CNN+LSTM+head; pose last-stage+BN+LSTM+head), LR=3e-4, cosine\")\nbest2 = train_epochs(train_dl, val_dl, epochs=30, lr=3e-4, wd=5e-4, cosine=True, desc=\"[PH2]\", save_path=PH2_PATH)\n\n# Phase 3 — fine-tune (freeze both encoders; train LSTMs + heads; BN frozen)\nfreeze_bn(model)\nset_requires_grad(model, False)\n\nset_requires_grad(model.land.lstm,   True)\nset_requires_grad(model.land.fc1,    True)\nset_requires_grad(model.land.fc_out, True)\n\nset_requires_grad(model.pose.lstm,   True)\nset_requires_grad(model.pose.fc1,    True)\nset_requires_grad(model.pose.fc_out, True)\n\nfor p in model.fuse.parameters(): p.requires_grad = True\n\nprint(\"Phase 3: fine-tune (LSTMs+heads), LR=5e-5\")\nbest3 = train_epochs(train_dl, val_dl, epochs=12, lr=5e-5, wd=5e-4, cosine=False, desc=\"[PH3]\", save_path=PH3_PATH)\n\n# Pick the best across phases and copy to a single final path\naccs  = [best1, best2, best3]\npaths = [PH1_PATH, PH2_PATH, PH3_PATH]\nbest_idx = int(np.argmax(accs))\nshutil.copy2(paths[best_idx], BEST_OVERALL_PATH)\n\nprint(\"Best val accuracy across phases:\", accs[best_idx])\nprint(\"Saved overall-best checkpoint to:\", BEST_OVERALL_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:09:48.185824Z","iopub.execute_input":"2025-11-10T11:09:48.186393Z","iopub.status.idle":"2025-11-10T12:17:31.155965Z","shell.execute_reply.started":"2025-11-10T11:09:48.186370Z","shell.execute_reply":"2025-11-10T12:17:31.155157Z"}},"outputs":[{"name":"stdout","text":"Phase 1: warm-up (land: conv2+head; pose: head), LR=1e-3\n[PH1] Epoch 01 | train loss 3.4999 acc 0.183 f1 0.167 | val acc 0.191 f1 0.139\n[PH1] Epoch 02 | train loss 2.4196 acc 0.469 f1 0.455 | val acc 0.181 f1 0.143\n[PH1] Epoch 03 | train loss 1.9980 acc 0.607 f1 0.600 | val acc 0.275 f1 0.227\n[PH1] Epoch 04 | train loss 1.7942 acc 0.699 f1 0.698 | val acc 0.343 f1 0.292\n[PH1] Epoch 05 | train loss 1.7211 acc 0.737 f1 0.733 | val acc 0.275 f1 0.244\n[PH1] Epoch 06 | train loss 1.6230 acc 0.774 f1 0.772 | val acc 0.387 f1 0.338\n[PH1] Epoch 07 | train loss 1.5378 acc 0.815 f1 0.812 | val acc 0.392 f1 0.336\n[PH1] Epoch 08 | train loss 1.5608 acc 0.792 f1 0.789 | val acc 0.422 f1 0.374\n[PH1] Epoch 09 | train loss 1.4746 acc 0.837 f1 0.837 | val acc 0.417 f1 0.373\n[PH1] Epoch 10 | train loss 1.4193 acc 0.848 f1 0.847 | val acc 0.338 f1 0.315\n[PH1] Epoch 11 | train loss 1.4297 acc 0.858 f1 0.860 | val acc 0.324 f1 0.291\n[PH1] Epoch 12 | train loss 1.4442 acc 0.851 f1 0.850 | val acc 0.373 f1 0.341\nPhase 2: joint (land CNN+LSTM+head; pose last-stage+BN+LSTM+head), LR=3e-4, cosine\n[PH2] Epoch 01 | train loss 1.8148 acc 0.701 f1 0.694 | val acc 0.309 f1 0.283\n[PH2] Epoch 02 | train loss 1.5180 acc 0.820 f1 0.823 | val acc 0.451 f1 0.415\n[PH2] Epoch 03 | train loss 1.4451 acc 0.852 f1 0.849 | val acc 0.451 f1 0.374\n[PH2] Epoch 04 | train loss 1.3423 acc 0.901 f1 0.901 | val acc 0.397 f1 0.342\n[PH2] Epoch 05 | train loss 1.2810 acc 0.922 f1 0.922 | val acc 0.397 f1 0.345\n[PH2] Epoch 06 | train loss 1.2659 acc 0.927 f1 0.927 | val acc 0.441 f1 0.370\n[PH2] Epoch 07 | train loss 1.2357 acc 0.931 f1 0.928 | val acc 0.456 f1 0.412\n[PH2] Epoch 08 | train loss 1.2054 acc 0.946 f1 0.947 | val acc 0.515 f1 0.457\n[PH2] Epoch 09 | train loss 1.1779 acc 0.954 f1 0.954 | val acc 0.456 f1 0.401\n[PH2] Epoch 10 | train loss 1.1653 acc 0.954 f1 0.952 | val acc 0.466 f1 0.413\n[PH2] Epoch 11 | train loss 1.1357 acc 0.971 f1 0.971 | val acc 0.500 f1 0.447\n[PH2] Epoch 12 | train loss 1.1272 acc 0.961 f1 0.963 | val acc 0.485 f1 0.455\n[PH2] Epoch 13 | train loss 1.1214 acc 0.969 f1 0.971 | val acc 0.441 f1 0.395\n[PH2] Epoch 14 | train loss 1.0992 acc 0.975 f1 0.974 | val acc 0.495 f1 0.454\n[PH2] Epoch 15 | train loss 1.0851 acc 0.977 f1 0.977 | val acc 0.436 f1 0.408\n[PH2] Epoch 16 | train loss 1.0797 acc 0.984 f1 0.984 | val acc 0.534 f1 0.494\n[PH2] Epoch 17 | train loss 1.0376 acc 0.989 f1 0.989 | val acc 0.480 f1 0.451\n[PH2] Epoch 18 | train loss 1.0376 acc 0.992 f1 0.993 | val acc 0.480 f1 0.450\n[PH2] Epoch 19 | train loss 1.0261 acc 0.990 f1 0.991 | val acc 0.466 f1 0.437\n[PH2] Epoch 20 | train loss 1.0181 acc 0.992 f1 0.993 | val acc 0.525 f1 0.490\n[PH2] Epoch 21 | train loss 1.0137 acc 0.996 f1 0.996 | val acc 0.534 f1 0.492\n[PH2] Epoch 22 | train loss 1.0045 acc 0.995 f1 0.996 | val acc 0.564 f1 0.515\n[PH2] Epoch 23 | train loss 1.0089 acc 0.993 f1 0.993 | val acc 0.554 f1 0.501\n[PH2] Epoch 24 | train loss 0.9979 acc 0.995 f1 0.995 | val acc 0.520 f1 0.472\n[PH2] Epoch 25 | train loss 0.9952 acc 0.994 f1 0.994 | val acc 0.529 f1 0.478\n[PH2] Epoch 26 | train loss 0.9915 acc 0.994 f1 0.995 | val acc 0.578 f1 0.525\n[PH2] Epoch 27 | train loss 0.9820 acc 0.997 f1 0.998 | val acc 0.407 f1 0.388\n[PH2] Epoch 28 | train loss 0.9939 acc 0.996 f1 0.995 | val acc 0.525 f1 0.493\n[PH2] Epoch 29 | train loss 0.9889 acc 0.995 f1 0.996 | val acc 0.593 f1 0.545\n[PH2] Epoch 30 | train loss 0.9959 acc 0.995 f1 0.996 | val acc 0.515 f1 0.482\nPhase 3: fine-tune (LSTMs+heads), LR=5e-5\n[PH3] Epoch 01 | train loss 0.9903 acc 0.993 f1 0.993 | val acc 0.569 f1 0.516\n[PH3] Epoch 02 | train loss 0.9876 acc 0.993 f1 0.994 | val acc 0.534 f1 0.484\n[PH3] Epoch 03 | train loss 0.9959 acc 0.992 f1 0.992 | val acc 0.603 f1 0.563\n[PH3] Epoch 04 | train loss 0.9849 acc 0.994 f1 0.995 | val acc 0.525 f1 0.492\n[PH3] Epoch 05 | train loss 0.9880 acc 0.993 f1 0.993 | val acc 0.578 f1 0.534\n[PH3] Epoch 06 | train loss 0.9765 acc 0.997 f1 0.997 | val acc 0.520 f1 0.480\n[PH3] Epoch 07 | train loss 0.9800 acc 0.997 f1 0.997 | val acc 0.500 f1 0.477\n[PH3] Epoch 08 | train loss 0.9714 acc 0.995 f1 0.996 | val acc 0.505 f1 0.434\n[PH3] Epoch 09 | train loss 0.9835 acc 0.993 f1 0.993 | val acc 0.583 f1 0.555\n[PH3] Epoch 10 | train loss 0.9901 acc 0.993 f1 0.993 | val acc 0.569 f1 0.521\n[PH3] Epoch 11 | train loss 0.9869 acc 0.995 f1 0.995 | val acc 0.500 f1 0.455\n[PH3] Epoch 12 | train loss 0.9640 acc 0.998 f1 0.998 | val acc 0.520 f1 0.482\nBest val accuracy across phases: 0.6029411764705882\nSaved overall-best checkpoint to: /kaggle/working/best_two_stream_overall.pth\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Cell 10 — Final test evaluation (best-by-val-acc)\nckpt = torch.load(\"/kaggle/working/best_two_stream.pth\", map_location=device)\nmodel.load_state_dict(ckpt['model'])\n\nfrom sklearn.metrics import accuracy_score, f1_score\ndef evaluate_final(m, dl):\n    m.eval(); preds,targs=[],[]\n    for xl,xp,y in dl:\n        xl=xl.to(device,non_blocking=True); xp=xp.to(device,non_blocking=True)\n        with torch.no_grad():\n            logits,_,_ = m(xl,xp)\n        preds.append(logits.argmax(1).cpu().numpy()); targs.append(y)\n    preds=np.concatenate(preds); targs=np.concatenate(targs)\n    acc=accuracy_score(targs,preds); f1=f1_score(targs,preds,average='macro',zero_division=0)\n    return acc,f1\n\ntest_acc, test_f1 = evaluate_final(model, test_dl)\nprint(f\"Test | acc {test_acc:.3f} macro-F1 {test_f1:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:17:42.196691Z","iopub.execute_input":"2025-11-10T12:17:42.196998Z","iopub.status.idle":"2025-11-10T12:17:51.229373Z","shell.execute_reply.started":"2025-11-10T12:17:42.196970Z","shell.execute_reply":"2025-11-10T12:17:51.228342Z"}},"outputs":[{"name":"stdout","text":"Test | acc 0.297 macro-F1 0.252\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}